{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAPSTONE_Chatbot_Ver01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb_5bl7G_n30",
        "outputId": "fe257dbf-6d20-4066-fa66-4d027ddddc80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.3.1\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import sys\n",
        "\n",
        "#!pip install tensorflow==2.3.1\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "#!pip install tensorflow-datasets==4.1.0\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Tensorflow version {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz2ZbpAREmRA",
        "outputId": "47e6fd9b-4e20-4545-bbed-bb2424344cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPLICAS: 1\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU {}'.format(tpu.cluster_spec().as_dict()['worker']))\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: {}\".format(strategy.num_replicas_in_sync))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alnYEatYcHp7"
      },
      "outputs": [],
      "source": [
        "# Maximum sentence length\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "# Maximum number of samples to preprocess\n",
        "MAX_SAMPLES = 50000\n",
        "\n",
        "# For tf.data.Dataset\n",
        "BATCH_SIZE = 64 * strategy.num_replicas_in_sync\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# For Transformer\n",
        "NUM_LAYERS = 2\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "UNITS = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "EPOCHS = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S17Nfn6W_vhd"
      },
      "outputs": [],
      "source": [
        "#Skip\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'cornell_movie_dialogs.zip',\n",
        "    origin=\n",
        "    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_dataset = os.path.join(\n",
        "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
        "\n",
        "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
        "path_to_movie_conversations = os.path.join(path_to_dataset,\n",
        "                                           'movie_conversations.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B147qKb_0ks"
      },
      "outputs": [],
      "source": [
        "#Skip\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "  sentence = sentence.lower().strip()\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "  # removing contractions\n",
        "  sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
        "  sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
        "  sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
        "  sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
        "  sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
        "  sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
        "  sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
        "  sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
        "  sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
        "  sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
        "  sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
        "  sentence = re.sub(r\"n't\", \" not\", sentence)\n",
        "  sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
        "  sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence\n",
        "\n",
        "\n",
        "def load_conversations():\n",
        "  # dictionary of line id to text\n",
        "  id2line = {}\n",
        "  with open(path_to_movie_lines, errors='ignore') as file:\n",
        "    lines = file.readlines()\n",
        "  for line in lines:\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "    id2line[parts[0]] = parts[4]\n",
        "\n",
        "  inputs, outputs = [], []\n",
        "  with open(path_to_movie_conversations, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "  for line in lines:\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "    # get conversation in a list of line ID\n",
        "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
        "    for i in range(len(conversation) - 1):\n",
        "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
        "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
        "      if len(inputs) >= MAX_SAMPLES:\n",
        "        return inputs, outputs\n",
        "  return inputs, outputs\n",
        "\n",
        "\n",
        "questions, answers = load_conversations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6XX2udMTCQt"
      },
      "outputs": [],
      "source": [
        "#Skip\n",
        "\n",
        "# Build tokenizer using tfds for both questions and answers\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    questions + answers, target_vocab_size=2**13)\n",
        "\n",
        "# Define start and end token to indicate the start and end of a sentence\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "\n",
        "# Vocabulary size plus start and end token\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YESTPgeg_XgT"
      },
      "outputs": [],
      "source": [
        "#SKIP\n",
        "\n",
        "# Tokenize, filter and pad sentences\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "  \n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # tokenize sentence\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "    # check tokenized sentence max length\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "      tokenized_inputs.append(sentence1)\n",
        "      tokenized_outputs.append(sentence2)\n",
        "  \n",
        "  # pad tokenized sentences\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  \n",
        "  return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "\n",
        "questions, answers = tokenize_and_filter(questions, answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pttC3XxgAXWQ"
      },
      "outputs": [],
      "source": [
        "#Skip\n",
        "\n",
        "# decoder inputs use the previous target as input\n",
        "# remove START_TOKEN from targets\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENfqAFna_50H"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  \"\"\"Calculate the attention weights. \"\"\"\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # add the mask to zero out padding tokens\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k)\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  output = tf.matmul(attention_weights, value)\n",
        "\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9eYssGIAG4h"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "  \n",
        "  def get_config(self):\n",
        "        config = super(MultiHeadAttention,self).get_config()\n",
        "        config.update({\n",
        "            'num_heads':self.num_heads,\n",
        "            'd_model':self.d_model,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.keras.layers.Lambda(lambda inputs:tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth)))(inputs)\n",
        "    return tf.keras.layers.Lambda(lambda inputs: tf.transpose(inputs, perm=[0, 2, 1, 3]))(inputs)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # linear layers\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # split heads\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    # scaled dot-product attention\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "    scaled_attention = tf.keras.layers.Lambda(lambda scaled_attention: tf.transpose(\n",
        "        scaled_attention, perm=[0, 2, 1, 3]))(scaled_attention)\n",
        "\n",
        "    # concatenation of heads\n",
        "    concat_attention = tf.keras.layers.Lambda(lambda scaled_attention: tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model)))(scaled_attention)\n",
        "\n",
        "    # final linear layer\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imCQ0jrvWhC7"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(x):\n",
        "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "  # (batch_size, 1, 1, sequence length)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSVdD2zKWaXx"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(x):\n",
        "  seq_len = tf.shape(x)[1]\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  padding_mask = create_padding_mask(x)\n",
        "  return tf.maximum(look_ahead_mask, padding_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9Oibz2es-qW"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "  \n",
        "  def get_config(self):\n",
        "\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update({\n",
        "            'position': self.position,\n",
        "            'd_model': self.d_model,\n",
        "            \n",
        "        })\n",
        "        return config\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "    # apply sin to even index in the array\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd index in the array\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5guJOLJmfcuX"
      },
      "outputs": [],
      "source": [
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  add_attention = tf.keras.layers.add([inputs,attention])\n",
        "  attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  add_attention = tf.keras.layers.add([attention,outputs])\n",
        "  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRfugon5Wy-Y"
      },
      "outputs": [],
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.keras.layers.Lambda(lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32)))(d_model)\n",
        "  embeddings = PositionalEncoding(vocab_size,d_model)(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mLvvNMWgDnf"
      },
      "outputs": [],
      "source": [
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "  attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "  add_attention = tf.keras.layers.add([attention1,inputs])    \n",
        "  attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
        "\n",
        "  attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "  add_attention = tf.keras.layers.add([attention2,attention1])\n",
        "  attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  add_attention = tf.keras.layers.add([outputs,attention2])\n",
        "  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYRx7YzCW4bu"
      },
      "outputs": [],
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name='decoder'):\n",
        "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "  \n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.keras.layers.Lambda(lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32)))(d_model)\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = decoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name='decoder_layer_{}'.format(i),\n",
        "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW-v7Fz6XAfC"
      },
      "outputs": [],
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "  # mask the future tokens for decoder inputs at the 1st attention block\n",
        "  look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "  # mask the encoder outputs for the 2nd attention block\n",
        "  dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "  enc_outputs = encoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  dec_outputs = decoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UInVM9iGAMv1"
      },
      "outputs": [],
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  \n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW3SeLDhAMJd"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = tf.constant(d_model,dtype=tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def get_config(self):\n",
        "        return {\"d_model\": self.d_model,\"warmup_steps\":self.warmup_steps}\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.multiply(tf.math.rsqrt(self.d_model), tf.math.minimum(arg1, arg2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QqojIa5WEQq",
        "outputId": "4c90604d-e79c-4c8a-ac5a-cfea0a3bbe2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "inputs (InputLayer)             [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Functional)            (None, None, 256)    1345536     inputs[0][0]                     \n",
            "                                                                 enc_padding_mask[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Functional)            (None, None, 256)    1872896     dec_inputs[0][0]                 \n",
            "                                                                 encoder[0][0]                    \n",
            "                                                                 look_ahead_mask[0][0]            \n",
            "                                                                 dec_padding_mask[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "outputs (Dense)                 (None, None, 1138)   292466      decoder[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,510,898\n",
            "Trainable params: 3,510,898\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# clear backend\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "# initialize and compile model within strategy scope\n",
        "with strategy.scope():\n",
        "  model = transformer(\n",
        "      vocab_size=VOCAB_SIZE,\n",
        "      num_layers=NUM_LAYERS,\n",
        "      units=UNITS,\n",
        "      d_model=D_MODEL,\n",
        "      num_heads=NUM_HEADS,\n",
        "      dropout=DROPOUT)\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7iahRzlLNG2",
        "outputId": "ecbbb511-6b4c-4f38-c7e3-080fdb32f291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "690/690 [==============================] - 1364s 2s/step - loss: 2.0416 - accuracy: 0.0461\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc531bbea90>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "#Skip\n",
        "\n",
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('test.csv', encoding='mac_roman', na_values=[])"
      ],
      "metadata": {
        "id": "YIBTimiqYXM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1xAMaKKwZAYF",
        "outputId": "a91115b5-df7e-4967-fc42-95a06c0043ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Question  \\\n",
              "0                     How do I reschedule my flight?   \n",
              "1     How to check flight reschedule policy via app?   \n",
              "2  How to check flight reschedule policy via desktop   \n",
              "3  Can I reschedule any flight through easy sched...   \n",
              "4  can i change the date, time, or route of my fl...   \n",
              "\n",
              "                                              Answer  \n",
              "0  1. Find your flight booking via My Booking in ...  \n",
              "1  If you are searching for a flight, here is the...  \n",
              "2  If you are searching for a flight, here is the...  \n",
              "3  Not every flight is supported by Easy Reschedu...  \n",
              "4  Some airlines allow changes for date, time, ro...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6fa8eb1a-b095-40e3-888c-45855de9efe3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How do I reschedule my flight?</td>\n",
              "      <td>1. Find your flight booking via My Booking in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How to check flight reschedule policy via app?</td>\n",
              "      <td>If you are searching for a flight, here is the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How to check flight reschedule policy via desktop</td>\n",
              "      <td>If you are searching for a flight, here is the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Can I reschedule any flight through easy sched...</td>\n",
              "      <td>Not every flight is supported by Easy Reschedu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>can i change the date, time, or route of my fl...</td>\n",
              "      <td>Some airlines allow changes for date, time, ro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fa8eb1a-b095-40e3-888c-45855de9efe3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6fa8eb1a-b095-40e3-888c-45855de9efe3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6fa8eb1a-b095-40e3-888c-45855de9efe3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Skip\n",
        "\n",
        "q = df['Question'].to_numpy(dtype='str')\n",
        "a = df['Answer'].to_numpy(dtype='str')"
      ],
      "metadata": {
        "id": "NRfqtKHrZ4HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Skip\n",
        "\n",
        "type(q[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CEulfB4aolx",
        "outputId": "ea291060-5c57-4e89-e8d1-f09cc5710130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.str_"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Question'].fillna('', inplace=True)\n",
        "df['Answer'].fillna('', inplace=True)"
      ],
      "metadata": {
        "id": "bsRECwcLer_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = df['Question'].values.tolist()\n",
        "a = df['Answer'].values.tolist()"
      ],
      "metadata": {
        "id": "aaz_tczGbQG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build tokenizer using tfds for both questions and answers\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    q + a, target_vocab_size=2**13)\n",
        "\n",
        "# Define start and end token to indicate the start and end of a sentence\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "\n",
        "# Vocabulary size plus start and end token\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2"
      ],
      "metadata": {
        "id": "we3WWRd8aAWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenized sample question: {}'.format(tokenizer.encode(q[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY4VPrLUe0ar",
        "outputId": "4f2924c5-9fc5-48d7-efee-ad29895f10c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sample question: [31, 403, 24, 26, 33, 45, 943]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize, filter and pad sentences\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "  \n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # tokenize sentence\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "    # check tokenized sentence max length\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "      tokenized_inputs.append(sentence1)\n",
        "      tokenized_outputs.append(sentence2)\n",
        "  \n",
        "  # pad tokenized sentences\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  \n",
        "  return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "\n",
        "questions, answers = tokenize_and_filter(q, a)\n",
        "print('Vocab size: {}'.format(VOCAB_SIZE))\n",
        "print('Number of samples: {}'.format(len(questions)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU0G1sdKe9PO",
        "outputId": "ce9f6699-301e-4496-ed06-98103eff85de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 1138\n",
            "Number of samples: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder inputs use the previous target as input\n",
        "# remove START_TOKEN from targets\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "WrECyfqyfD0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd7w60v-fUK-",
        "outputId": "93cd3527-a9ae-4d4b-899f-32ca68fc3bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1329 - accuracy: 0.6259\n",
            "Epoch 2/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1334 - accuracy: 0.6247\n",
            "Epoch 3/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1280 - accuracy: 0.6247\n",
            "Epoch 4/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1308 - accuracy: 0.6247\n",
            "Epoch 5/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1260 - accuracy: 0.6259\n",
            "Epoch 6/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1244 - accuracy: 0.6247\n",
            "Epoch 7/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1237 - accuracy: 0.6259\n",
            "Epoch 8/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1225 - accuracy: 0.6259\n",
            "Epoch 9/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1193 - accuracy: 0.6259\n",
            "Epoch 10/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1188 - accuracy: 0.6247\n",
            "Epoch 11/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1152 - accuracy: 0.6259\n",
            "Epoch 12/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1159 - accuracy: 0.6259\n",
            "Epoch 13/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1138 - accuracy: 0.6247\n",
            "Epoch 14/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1105 - accuracy: 0.6259\n",
            "Epoch 15/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1087 - accuracy: 0.6247\n",
            "Epoch 16/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1065 - accuracy: 0.6247\n",
            "Epoch 17/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1068 - accuracy: 0.6247\n",
            "Epoch 18/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1043 - accuracy: 0.6259\n",
            "Epoch 19/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1042 - accuracy: 0.6259\n",
            "Epoch 20/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0997 - accuracy: 0.6259\n",
            "Epoch 21/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0998 - accuracy: 0.6259\n",
            "Epoch 22/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0966 - accuracy: 0.6259\n",
            "Epoch 23/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0948 - accuracy: 0.6259\n",
            "Epoch 24/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.6259\n",
            "Epoch 25/250\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0917 - accuracy: 0.6259\n",
            "Epoch 26/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0905 - accuracy: 0.6259\n",
            "Epoch 27/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0888 - accuracy: 0.6247\n",
            "Epoch 28/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0880 - accuracy: 0.6259\n",
            "Epoch 29/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0867 - accuracy: 0.6259\n",
            "Epoch 30/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0896 - accuracy: 0.6259\n",
            "Epoch 31/250\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0834 - accuracy: 0.6259\n",
            "Epoch 32/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0856 - accuracy: 0.6259\n",
            "Epoch 33/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.6259\n",
            "Epoch 34/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.6259\n",
            "Epoch 35/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0805 - accuracy: 0.6247\n",
            "Epoch 36/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0792 - accuracy: 0.6259\n",
            "Epoch 37/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.6259\n",
            "Epoch 38/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.6259\n",
            "Epoch 39/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 0.6259\n",
            "Epoch 40/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.6259\n",
            "Epoch 41/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0713 - accuracy: 0.6259\n",
            "Epoch 42/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.6259\n",
            "Epoch 43/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.6247\n",
            "Epoch 44/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0697 - accuracy: 0.6259\n",
            "Epoch 45/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0691 - accuracy: 0.6259\n",
            "Epoch 46/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0685 - accuracy: 0.6259\n",
            "Epoch 47/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0659 - accuracy: 0.6259\n",
            "Epoch 48/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0656 - accuracy: 0.6259\n",
            "Epoch 49/250\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0665 - accuracy: 0.6259\n",
            "Epoch 50/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0618 - accuracy: 0.6259\n",
            "Epoch 51/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.6259\n",
            "Epoch 52/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.6259\n",
            "Epoch 53/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0607 - accuracy: 0.6259\n",
            "Epoch 54/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.6259\n",
            "Epoch 55/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0595 - accuracy: 0.6259\n",
            "Epoch 56/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0586 - accuracy: 0.6259\n",
            "Epoch 57/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0563 - accuracy: 0.6259\n",
            "Epoch 58/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.6259\n",
            "Epoch 59/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.6259\n",
            "Epoch 60/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.6259\n",
            "Epoch 61/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.6247\n",
            "Epoch 62/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0531 - accuracy: 0.6259\n",
            "Epoch 63/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0531 - accuracy: 0.6259\n",
            "Epoch 64/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0526 - accuracy: 0.6259\n",
            "Epoch 65/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.6259\n",
            "Epoch 66/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 0.6259\n",
            "Epoch 67/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0496 - accuracy: 0.6259\n",
            "Epoch 68/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0507 - accuracy: 0.6259\n",
            "Epoch 69/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0469 - accuracy: 0.6259\n",
            "Epoch 70/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.6259\n",
            "Epoch 71/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.6259\n",
            "Epoch 72/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.6259\n",
            "Epoch 73/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0445 - accuracy: 0.6259\n",
            "Epoch 74/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0462 - accuracy: 0.6259\n",
            "Epoch 75/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.6259\n",
            "Epoch 76/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 0.6259\n",
            "Epoch 77/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0434 - accuracy: 0.6259\n",
            "Epoch 78/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0430 - accuracy: 0.6259\n",
            "Epoch 79/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0418 - accuracy: 0.6259\n",
            "Epoch 80/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0411 - accuracy: 0.6259\n",
            "Epoch 81/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 0.6259\n",
            "Epoch 82/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0399 - accuracy: 0.6259\n",
            "Epoch 83/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 0.6259\n",
            "Epoch 84/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0382 - accuracy: 0.6259\n",
            "Epoch 85/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0388 - accuracy: 0.6259\n",
            "Epoch 86/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0377 - accuracy: 0.6259\n",
            "Epoch 87/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.6259\n",
            "Epoch 88/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0361 - accuracy: 0.6259\n",
            "Epoch 89/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0373 - accuracy: 0.6259\n",
            "Epoch 90/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0375 - accuracy: 0.6259\n",
            "Epoch 91/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0356 - accuracy: 0.6259\n",
            "Epoch 92/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0346 - accuracy: 0.6259\n",
            "Epoch 93/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0340 - accuracy: 0.6259\n",
            "Epoch 94/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0333 - accuracy: 0.6259\n",
            "Epoch 95/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0343 - accuracy: 0.6259\n",
            "Epoch 96/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 0.6259\n",
            "Epoch 97/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.6259\n",
            "Epoch 98/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.6259\n",
            "Epoch 99/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.6259\n",
            "Epoch 100/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0318 - accuracy: 0.6259\n",
            "Epoch 101/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0311 - accuracy: 0.6259\n",
            "Epoch 102/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.6259\n",
            "Epoch 103/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.6259\n",
            "Epoch 104/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.6259\n",
            "Epoch 105/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 0.6259\n",
            "Epoch 106/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0287 - accuracy: 0.6259\n",
            "Epoch 107/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.6259\n",
            "Epoch 108/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.6259\n",
            "Epoch 109/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.6259\n",
            "Epoch 110/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0273 - accuracy: 0.6259\n",
            "Epoch 111/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0278 - accuracy: 0.6259\n",
            "Epoch 112/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0259 - accuracy: 0.6259\n",
            "Epoch 113/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0264 - accuracy: 0.6259\n",
            "Epoch 114/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.6259\n",
            "Epoch 115/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.6259\n",
            "Epoch 116/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0251 - accuracy: 0.6259\n",
            "Epoch 117/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.6259\n",
            "Epoch 118/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.6259\n",
            "Epoch 119/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.6259\n",
            "Epoch 120/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.6259\n",
            "Epoch 121/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.6259\n",
            "Epoch 122/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 0.6259\n",
            "Epoch 123/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0234 - accuracy: 0.6259\n",
            "Epoch 124/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.6259\n",
            "Epoch 125/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.6259\n",
            "Epoch 126/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.6259\n",
            "Epoch 127/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0215 - accuracy: 0.6259\n",
            "Epoch 128/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.6259\n",
            "Epoch 129/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0211 - accuracy: 0.6259\n",
            "Epoch 130/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0214 - accuracy: 0.6259\n",
            "Epoch 131/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.6259\n",
            "Epoch 132/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.6259\n",
            "Epoch 133/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.6259\n",
            "Epoch 134/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.6259\n",
            "Epoch 135/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0194 - accuracy: 0.6259\n",
            "Epoch 136/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.6259\n",
            "Epoch 137/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.6259\n",
            "Epoch 138/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.6259\n",
            "Epoch 139/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0191 - accuracy: 0.6259\n",
            "Epoch 140/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.6259\n",
            "Epoch 141/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.6259\n",
            "Epoch 142/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.6259\n",
            "Epoch 143/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0173 - accuracy: 0.6259\n",
            "Epoch 144/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.6259\n",
            "Epoch 145/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0168 - accuracy: 0.6259\n",
            "Epoch 146/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0171 - accuracy: 0.6259\n",
            "Epoch 147/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0163 - accuracy: 0.6259\n",
            "Epoch 148/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0164 - accuracy: 0.6259\n",
            "Epoch 149/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 0.6259\n",
            "Epoch 150/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0166 - accuracy: 0.6259\n",
            "Epoch 151/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.6259\n",
            "Epoch 152/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0157 - accuracy: 0.6259\n",
            "Epoch 153/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.6259\n",
            "Epoch 154/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.6259\n",
            "Epoch 155/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.6259\n",
            "Epoch 156/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.6259\n",
            "Epoch 157/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.6259\n",
            "Epoch 158/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.6259\n",
            "Epoch 159/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 0.6259\n",
            "Epoch 160/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.6259\n",
            "Epoch 161/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0142 - accuracy: 0.6259\n",
            "Epoch 162/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 0.6259\n",
            "Epoch 163/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 0.6259\n",
            "Epoch 164/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 0.6259\n",
            "Epoch 165/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 0.6259\n",
            "Epoch 166/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 0.6259\n",
            "Epoch 167/250\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0132 - accuracy: 0.6259\n",
            "Epoch 168/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 0.6259\n",
            "Epoch 169/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 0.6259\n",
            "Epoch 170/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0129 - accuracy: 0.6259\n",
            "Epoch 171/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.6259\n",
            "Epoch 172/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.6259\n",
            "Epoch 173/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.6259\n",
            "Epoch 174/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.6259\n",
            "Epoch 175/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.6259\n",
            "Epoch 176/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0122 - accuracy: 0.6259\n",
            "Epoch 177/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0119 - accuracy: 0.6259\n",
            "Epoch 178/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.6259\n",
            "Epoch 179/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.6259\n",
            "Epoch 180/250\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.6259\n",
            "Epoch 181/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 0.6259\n",
            "Epoch 182/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0107 - accuracy: 0.6259\n",
            "Epoch 183/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 0.6259\n",
            "Epoch 184/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0107 - accuracy: 0.6259\n",
            "Epoch 185/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0108 - accuracy: 0.6259\n",
            "Epoch 186/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.6259\n",
            "Epoch 187/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0100 - accuracy: 0.6259\n",
            "Epoch 188/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.6259\n",
            "Epoch 189/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.6259\n",
            "Epoch 190/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.6259\n",
            "Epoch 191/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.6259\n",
            "Epoch 192/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0096 - accuracy: 0.6259\n",
            "Epoch 193/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.6259\n",
            "Epoch 194/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.6259\n",
            "Epoch 195/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.6259\n",
            "Epoch 196/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 0.6259\n",
            "Epoch 197/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.6259\n",
            "Epoch 198/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.6259\n",
            "Epoch 199/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.6259\n",
            "Epoch 200/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0085 - accuracy: 0.6259\n",
            "Epoch 201/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 0.6259\n",
            "Epoch 202/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 0.6259\n",
            "Epoch 203/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 0.6259\n",
            "Epoch 204/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0084 - accuracy: 0.6259\n",
            "Epoch 205/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 0.6259\n",
            "Epoch 206/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0080 - accuracy: 0.6259\n",
            "Epoch 207/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 0.6259\n",
            "Epoch 208/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0080 - accuracy: 0.6259\n",
            "Epoch 209/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 0.6259\n",
            "Epoch 210/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.6259\n",
            "Epoch 211/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 0.6259\n",
            "Epoch 212/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 0.6259\n",
            "Epoch 213/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0072 - accuracy: 0.6259\n",
            "Epoch 214/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 0.6259\n",
            "Epoch 215/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.6259\n",
            "Epoch 216/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.6259\n",
            "Epoch 217/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0073 - accuracy: 0.6259\n",
            "Epoch 218/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0070 - accuracy: 0.6259\n",
            "Epoch 219/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0072 - accuracy: 0.6259\n",
            "Epoch 220/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 0.6259\n",
            "Epoch 221/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 0.6259\n",
            "Epoch 222/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 0.6259\n",
            "Epoch 223/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0066 - accuracy: 0.6259\n",
            "Epoch 224/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 0.6259\n",
            "Epoch 225/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 0.6259\n",
            "Epoch 226/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0063 - accuracy: 0.6259\n",
            "Epoch 227/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0063 - accuracy: 0.6259\n",
            "Epoch 228/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 0.6259\n",
            "Epoch 229/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.6259\n",
            "Epoch 230/250\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0060 - accuracy: 0.6259\n",
            "Epoch 231/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0059 - accuracy: 0.6259\n",
            "Epoch 232/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0060 - accuracy: 0.6259\n",
            "Epoch 233/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0056 - accuracy: 0.6259\n",
            "Epoch 234/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.6259\n",
            "Epoch 235/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.6259\n",
            "Epoch 236/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0056 - accuracy: 0.6259\n",
            "Epoch 237/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0055 - accuracy: 0.6259\n",
            "Epoch 238/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0055 - accuracy: 0.6259\n",
            "Epoch 239/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0057 - accuracy: 0.6259\n",
            "Epoch 240/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.6259\n",
            "Epoch 241/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0054 - accuracy: 0.6259\n",
            "Epoch 242/250\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 0.6259\n",
            "Epoch 243/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0051 - accuracy: 0.6259\n",
            "Epoch 244/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.6259\n",
            "Epoch 245/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0050 - accuracy: 0.6259\n",
            "Epoch 246/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.6259\n",
            "Epoch 247/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.6259\n",
            "Epoch 248/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0049 - accuracy: 0.6259\n",
            "Epoch 249/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0051 - accuracy: 0.6259\n",
            "Epoch 250/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.6259\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7de6c115d0>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "  sentence = sentence.lower().strip()\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "  # removing contractions\n",
        "  sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
        "  sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
        "  sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
        "  sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
        "  sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
        "  sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
        "  sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
        "  sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
        "  sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
        "  sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
        "  sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
        "  sentence = re.sub(r\"n't\", \" not\", sentence)\n",
        "  sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
        "  sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence\n",
        "\n",
        "def evaluate(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "  output = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "  for i in range(MAX_LENGTH):\n",
        "    predictions = model(inputs=[sentence, output], training=False)\n",
        "\n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[:, -1:, :]\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "\n",
        "    # concatenated the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0)\n",
        "\n",
        "\n",
        "def predict(sentence):\n",
        "  prediction = evaluate(sentence)\n",
        "\n",
        "  predicted_sentence = tokenizer.decode(\n",
        "      [i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Output: {}'.format(predicted_sentence))\n",
        "\n",
        "  return predicted_sentence"
      ],
      "metadata": {
        "id": "rnOe49mNfyDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = predict('How much Flight Refund Amount Deductions ?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQaFM3gofzNf",
        "outputId": "dc7e7669-9387-4263-ddf0-1cc6743cf476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: How much Flight Refund Amount Deductions ?\n",
            "Output: If you face problem during submit refund in Traveloka Apps, you can tell us by clicking this link to Contact Us and describe your problem.\n"
          ]
        }
      ]
    }
  ]
}